# ğŸ“– Estudo BÃ­blico com IA - Guia de InstalaÃ§Ã£o

## ğŸš€ InstalaÃ§Ã£o AutomÃ¡tica (Recomendado)

### Windows

1. **Execute o instalador com privilÃ©gios de administrador:**
   - Clique com o botÃ£o direito em `setup.bat`
   - Selecione "Executar como administrador"
   - Aguarde a conclusÃ£o da instalaÃ§Ã£o

2. **Inicie a aplicaÃ§Ã£o:**
   - DÃª um duplo clique em `start_app.bat`
   - A aplicaÃ§Ã£o abrirÃ¡ automaticamente no navegador

### O que o `setup.bat` faz?

âœ… Verifica e instala Python 3.11.9  
âœ… Verifica e instala Git  
âœ… Cria ambiente virtual Python  
âœ… Instala todas as dependÃªncias do `requirements.txt`  
âœ… Verifica e instala Ollama  
âœ… Baixa o modelo `llama3.2:1b`  
âœ… Configura arquivo `.env` com variÃ¡veis padrÃ£o  

---

## ğŸ› ï¸ InstalaÃ§Ã£o Manual

### PrÃ©-requisitos

- **Python 3.11+**: [Download](https://www.python.org/downloads/)
- **Git** (opcional): [Download](https://git-scm.com/downloads)
- **Ollama**: [Download](https://ollama.com/download)

### Passo a Passo

1. **Clone ou baixe o projeto**
   ```bash
   git clone <seu-repositorio>
   cd Biblia
   ```

2. **Crie o ambiente virtual**
   ```bash
   python -m venv .venv
   ```

3. **Ative o ambiente virtual**
   - Windows: `.venv\Scripts\activate`
   - Linux/Mac: `source .venv/bin/activate`

4. **Instale as dependÃªncias**
   ```bash
   pip install -r requirements.txt
   ```

5. **Configure o Ollama**
   ```bash
   # Inicie o servidor
   ollama serve
   
   # Em outro terminal, baixe o modelo
   ollama pull llama3.2:1b
   ```

6. **Configure o arquivo .env**
   ```env
   OLLAMA_BASE=http://127.0.0.1:11434
   OLLAMA_GENERATE_PATHS=/api/generate
   OLLAMA_MODEL_DEFAULT=llama3.2:1b
   STREAMLIT_SERVER_PORT=8501
   STREAMLIT_SERVER_ADDRESS=localhost
   ```

7. **Inicie a aplicaÃ§Ã£o**
   ```bash
   streamlit run app.py
   ```

---

## ğŸ¯ Usando a AplicaÃ§Ã£o

### 1ï¸âƒ£ Importar Dados BÃ­blicos

- Acesse a aba **"ğŸ“¥ Importar Dados"**
- Configure o repositÃ³rio GitHub (padrÃ£o jÃ¡ fornecido)
- Clique em **"Baixar e converter do GitHub"**
- Aguarde o download e conversÃ£o

### 2ï¸âƒ£ Leitura & Exegese

- Selecione a versÃ£o da BÃ­blia
- Escolha livro, capÃ­tulo e versÃ­culos
- Clique em **"âœ¨ Gerar ExplicaÃ§Ã£o"**
- O estudo serÃ¡ salvo automaticamente no histÃ³rico

### 3ï¸âƒ£ Gerador de SermÃµes

- Selecione a passagem bÃ­blica
- Preencha tema e tipo de pÃºblico
- Adicione notas adicionais (opcional)
- Clique em **"âœ¨ Gerar EsboÃ§o de SermÃ£o"**
- Acesse **"ğŸ“‹ HistÃ³rico SermÃµes"** para revisar

### 4ï¸âƒ£ Devocional & MeditaÃ§Ã£o

- Selecione um versÃ­culo
- Escolha o sentimento/tema (ex: "GratidÃ£o", "Paz")
- Clique em **"âœ¨ Gerar Devocional"**
- Acesse **"ğŸ•Šï¸ HistÃ³rico Devocionais"** para revisar

### 5ï¸âƒ£ Chat TeolÃ³gico

- Selecione um versÃ­culo base
- Digite sua pergunta teolÃ³gica
- Clique em **"âœ¨ Enviar Pergunta"**
- Acesse **"ğŸ’­ HistÃ³rico Chat"** para revisar conversas

---

## ğŸ”§ SoluÃ§Ã£o de Problemas

### Ollama Offline

```bash
# Verifique se Ollama estÃ¡ rodando
curl http://localhost:11434/api/version

# Se nÃ£o estiver, inicie manualmente
ollama serve
```

### Modelo nÃ£o encontrado

```bash
# Liste modelos instalados
ollama list

# Baixe o modelo padrÃ£o
ollama pull llama3.2:1b
```

### Erro de importaÃ§Ã£o Python

```bash
# Certifique-se que o ambiente virtual estÃ¡ ativo
.venv\Scripts\activate

# Reinstale as dependÃªncias
pip install -r requirements.txt --force-reinstall
```

### Port 8501 jÃ¡ em uso

- Modifique a porta no arquivo `.env`:
  ```env
  STREAMLIT_SERVER_PORT=8502
  ```
- Ou force outra porta:
  ```bash
  streamlit run app.py --server.port 8502
  ```

---

## ğŸ“¦ Estrutura do Projeto

```
Biblia/
â”œâ”€â”€ app.py                    # AplicaÃ§Ã£o principal
â”œâ”€â”€ bible_data_importer.py    # Importador de dados bÃ­blicos
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ .env                      # VariÃ¡veis de ambiente
â”œâ”€â”€ setup.bat                 # Instalador automÃ¡tico
â”œâ”€â”€ start_app.bat             # Inicializador da aplicaÃ§Ã£o
â”œâ”€â”€ bible_data.json           # Dados bÃ­blicos (apÃ³s importar)
â”œâ”€â”€ .venv/                    # Ambiente virtual Python
â””â”€â”€ INSTALL.md                # Este arquivo
```

---

## ğŸŒ Recursos Adicionais

- **Ollama**: https://ollama.com
- **Streamlit**: https://streamlit.io
- **Python**: https://www.python.org
- **Modelos Ollama**: https://ollama.com/library

---

## ğŸ“ Notas

- A primeira execuÃ§Ã£o pode demorar devido ao download do modelo LLM (~1.3GB)
- O Ollama precisa estar rodando sempre que usar a aplicaÃ§Ã£o
- Todos os histÃ³ricos sÃ£o salvos na sessÃ£o atual (em memÃ³ria)
- Para persistÃªncia permanente, serÃ¡ necessÃ¡rio adicionar salvamento em arquivo/banco

---

## ğŸ’¡ Dicas

1. **Modelos alternativos**: VocÃª pode usar outros modelos modificando o `.env`:
   ```env
   OLLAMA_MODEL_DEFAULT=llama3.2:3b
   OLLAMA_MODEL_DEFAULT=mistral
   ```

2. **Melhorar respostas**: Modelos maiores geram respostas melhores, mas sÃ£o mais lentos:
   - `llama3.2:1b` - RÃ¡pido, respostas bÃ¡sicas
   - `llama3.2:3b` - Balanceado
   - `llama3.1:8b` - Melhor qualidade

3. **Performance**: Se estiver lento, reduza o `max_tokens` no cÃ³digo

---

## ğŸ¤ Suporte

Para problemas ou dÃºvidas:
1. Verifique a seÃ§Ã£o "SoluÃ§Ã£o de Problemas"
2. Consulte os logs do terminal
3. Teste a conectividade do Ollama com `curl http://localhost:11434/api/tags`

**Feito com â¤ï¸ para o estudo da Palavra**
