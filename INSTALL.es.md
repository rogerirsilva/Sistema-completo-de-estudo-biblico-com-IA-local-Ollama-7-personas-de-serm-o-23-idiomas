# üìñ Estudio B√≠blico con IA - Gu√≠a de Instalaci√≥n

## üöÄ Instalaci√≥n Autom√°tica (Recomendado)

### Windows

1. **Ejecuta el instalador con privilegios de administrador:**
   - Haz clic derecho en `setup.bat`
   - Selecciona "Ejecutar como administrador"
   - Espera a que finalice la instalaci√≥n

2. **Inicia la aplicaci√≥n:**
   - Haz doble clic en `start_app.bat`
   - La aplicaci√≥n se abrir√° autom√°ticamente en el navegador

### ¬øQu√© hace `setup.bat`?

‚úÖ Verifica e instala Python 3.11.9  
‚úÖ Verifica e instala Git  
‚úÖ Crea entorno virtual Python  
‚úÖ Instala todas las dependencias de `requirements.txt`  
‚úÖ Verifica e instala Ollama  
‚úÖ Descarga el modelo `llama3.2:1b`  
‚úÖ Configura archivo `.env` con variables predeterminadas  

---

## üõ†Ô∏è Instalaci√≥n Manual

### Prerrequisitos

- **Python 3.11+**: [Descargar](https://www.python.org/downloads/)
- **Git** (opcional): [Descargar](https://git-scm.com/downloads)
- **Ollama**: [Descargar](https://ollama.com/download)

### Paso a Paso

1. **Clona o descarga el proyecto**
   ```bash
   git clone https://github.com/rogerirsilva/Biblia-em-23-Idiomas-Local-Com-IA-Ollama.git
   cd Biblia-em-23-Idiomas-Local-Com-IA-Ollama
   ```

2. **Crea el entorno virtual**
   ```bash
   python -m venv .venv
   ```

3. **Activa el entorno virtual**
   - Windows: `.venv\Scripts\activate`
   - Linux/Mac: `source .venv/bin/activate`

4. **Instala las dependencias**
   ```bash
   pip install -r requirements.txt
   ```

5. **Configura Ollama**
   ```bash
   # Inicia el servidor
   ollama serve
   
   # En otra terminal, descarga el modelo
   ollama pull llama3.2:1b
   ```

6. **Configura el archivo .env**
   ```env
   OLLAMA_BASE=http://127.0.0.1:11434
   OLLAMA_GENERATE_PATHS=/api/generate
   OLLAMA_MODEL_DEFAULT=llama3.2:1b
   STREAMLIT_SERVER_PORT=8501
   STREAMLIT_SERVER_ADDRESS=localhost
   ```

7. **Inicia la aplicaci√≥n**
   ```bash
   streamlit run app.py
   ```

---

## üéØ Usando la Aplicaci√≥n

### 1Ô∏è‚É£ Importar Datos B√≠blicos

- Accede a la pesta√±a **"üì• Importar Datos"**
- La aplicaci√≥n soporta importar versiones b√≠blicas desde archivos JSON
- Coloca archivos JSON en la carpeta `Dados_Json/{idioma}/`
- Haz clic en **"üîÑ Importar Versiones de la Carpeta"**

**Fuentes recomendadas de Biblias JSON:**
- **Portugu√©s**: [github.com/thiagobodruk/bible](https://github.com/thiagobodruk/bible)
- **Ingl√©s**: [github.com/scrollmapper/bible_databases](https://github.com/scrollmapper/bible_databases)
- **Espa√±ol**: [github.com/thiagobodruk/bible (rama es)](https://github.com/thiagobodruk/bible)

### 2Ô∏è‚É£ Lectura y Ex√©gesis

- Selecciona la versi√≥n de la Biblia
- Elige libro, cap√≠tulo y vers√≠culos
- Haz clic en **"‚ú® Generar Explicaci√≥n"**
- El estudio se guardar√° autom√°ticamente en el historial

### 3Ô∏è‚É£ Generador de Sermones

- Elige una de las 7 personas de serm√≥n
- Define tema, audiencia y notas
- Selecciona alcance (libro, m√∫ltiples libros, testamento o Biblia completa)
- Selecciona vers√≠culos espec√≠ficos
- Haz clic en **"‚ú® Generar Serm√≥n"**
- Exporta a PDF cuando est√© listo

### 4Ô∏è‚É£ Devocional y Meditaci√≥n

- Elige sentimiento/tema (Gratitud, Paz, Fuerza, etc.)
- Selecciona libro y vers√≠culos
- Genera devocional personalizado
- Accede al historial para revisar devocionales anteriores

### 5Ô∏è‚É£ Chat Teol√≥gico

- Haz preguntas sobre vers√≠culos espec√≠ficos
- Obt√©n an√°lisis teol√≥gico, hist√≥rico y cultural profundo
- Todas las conversaciones se guardan en el historial

### 6Ô∏è‚É£ Generador de Preguntas

- Genera de 1 a 50 preguntas b√≠blicas
- Elige modo: Con Respuestas o Solo Preguntas
- Perfecto para escuelas b√≠blicas y grupos de estudio
- Exporta a PDF

---

## üîß Soluci√≥n de Problemas

### ‚ùå Python no reconocido

**Soluci√≥n:**
- Aseg√∫rate de que Python est√© instalado
- Agrega Python al PATH:
  - Durante la instalaci√≥n, marca "Add Python to PATH"
  - O agrega manualmente: `C:\Users\{Usuario}\AppData\Local\Programs\Python\Python311`

### ‚ùå Ollama no conecta

**Soluci√≥n:**
```bash
# Verifica si Ollama est√° ejecut√°ndose
curl http://localhost:11434/api/version

# Inicia Ollama manualmente
ollama serve

# Windows: Verifica si est√° ejecut√°ndose
tasklist | findstr ollama
```

### ‚ùå Modelo no encontrado

**Soluci√≥n:**
```bash
# Lista modelos instalados
ollama list

# Descarga modelo recomendado
ollama pull llama3.2:1b

# Prueba el modelo
ollama run llama3.2:1b "Hola, ¬øc√≥mo est√°s?"
```

### ‚ùå Error al importar versiones b√≠blicas

**Soluci√≥n:**
```bash
# Activa el entorno virtual
.venv\Scripts\activate

# Reinstala dependencias
pip install -r requirements.txt --force-reinstall

# Verifica estructura de carpetas
dir Dados_Json\es
```

### ‚ùå ChromaDB no guarda datos

**Soluci√≥n:**
```bash
# Verifica permisos de carpeta chroma_db
icacls chroma_db

# Recrea base de datos
rmdir /s /q chroma_db
python -c "import chromadb; chromadb.Client()"
```

### ‚ùå Puerto 8501 ya en uso

**Soluci√≥n:**
```bash
# Termina proceso usando puerto 8501
# Windows:
netstat -ano | findstr :8501
taskkill /PID <PID> /F

# O cambia puerto en .env
STREAMLIT_SERVER_PORT=8502
```

---

## üêõ Errores Comunes

### ImportError: No module named 'streamlit'

**Soluci√≥n:**
```bash
# Aseg√∫rate de que el entorno virtual est√© activado
.venv\Scripts\activate

# Reinstala Streamlit
pip install streamlit --upgrade
```

### Connection refused to Ollama

**Soluci√≥n:**
1. Verifica si Ollama est√° instalado: `ollama --version`
2. Inicia servicio Ollama: `ollama serve`
3. Prueba conexi√≥n: `curl http://localhost:11434/api/version`

### Timeout al generar serm√≥n

**Soluci√≥n:**
- ‚úÖ El sistema ahora usa **timeout din√°mico** (hasta 10 minutos)
- ‚úÖ Continuaci√≥n autom√°tica para sermones complejos
- Si sigue fallando, prueba un modelo m√°s peque√±o: `llama3.2:1b`

---

## üì¶ Requisitos del Sistema

### M√≠nimo

- **SO**: Windows 10/11, Linux, macOS
- **RAM**: 4 GB
- **Almacenamiento**: 10 GB libres
- **Internet**: Solo para descarga inicial de modelos

### Recomendado

- **SO**: Windows 11 o Linux
- **RAM**: 8 GB o m√°s
- **Almacenamiento**: 20 GB libres (para m√∫ltiples versiones b√≠blicas)
- **GPU**: GPU NVIDIA para mejor rendimiento (opcional)

---

## üöÄ Consejos de Rendimiento

### 1. Elige el modelo adecuado

```bash
# M√°s r√°pido (2GB RAM)
ollama pull llama3.2:1b

# Balanceado (5GB RAM)
ollama pull llama3.2:3b

# Mejor calidad (8GB RAM)
ollama pull mistral

# M√°xima calidad (16GB RAM)
ollama pull llama3.1:8b
```

### 2. Habilitar GPU (solo NVIDIA)

```bash
# Verifica si GPU es detectada
ollama run llama3.2:1b --verbose

# Instala drivers CUDA si es necesario
# https://developer.nvidia.com/cuda-downloads
```

### 3. Ajustar timeout

En `app.py`, ajusta valores de timeout si es necesario:
```python
OLLAMA_REQUEST_TIMEOUT = 600  # 10 minutos
```

---

## üÜò Obtener Ayuda

- üêõ **Reportar errores**: [GitHub Issues](https://github.com/rogerirsilva/Biblia-em-23-Idiomas-Local-Com-IA-Ollama/issues)
- üí¨ **Discusiones**: [GitHub Discussions](https://github.com/rogerirsilva/Biblia-em-23-Idiomas-Local-Com-IA-Ollama/discussions)
- üìß **Contacto**: Abre un issue en GitHub

---

## üìö Recursos Adicionales

- [Documentaci√≥n Ollama](https://github.com/ollama/ollama/blob/main/docs/README.md)
- [Documentaci√≥n Streamlit](https://docs.streamlit.io/)
- [Documentaci√≥n ChromaDB](https://docs.trychroma.com/)
- [Documentaci√≥n ReportLab](https://www.reportlab.com/docs/)

---

**Hecho con ‚ù§Ô∏è para la gloria de Dios**

*"L√°mpara es a mis pies tu palabra, y lumbrera a mi camino." - Salmos 119:105*
